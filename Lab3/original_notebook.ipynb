{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "claAHmTTZe9E"
      },
      "source": [
        "# Lab 3: Fewshot ICL\n",
        "\n",
        "As knowledge graph requires background in SPARQL and/or LLM finetuning, this lab won't be totally related to what you saw in today's course.\n",
        "\n",
        "We'll be delving into In Context Learning (ICL), in particular ICL fewshot, and trying to understand how it works and when to use it. To do this, we'll be using the Transformer Library, a Mistral LLM and an emotion classification dataset.\n",
        "\n",
        "\n",
        "The laboratory is divided into 4 sections:\n",
        "0. Setup: This section is dedicated to installing modules, loading models and loading data.You don't need to code, just run it.\n",
        "1. Zeroshot Classification: Some of you may have had trouble finding a prompt that always returned a “well-formed” answer in the last lab. In this section, we'll use a “well-formed” prompt to perform zeroshot classification.\n",
        "2. Fewshot Classification - Random Retrieval: One of the most common methods of improving ICL classification is to add demonstrations to the prompt. This helps the LLM to “properly format” the response and can also give semantic information about how to solve the task. In this section, we will use random retrieved demonstration and compare the results with those of section 1.\n",
        "3. Fewshot Classification - Vector-based Retrieval: Extracting random demonstrations in fewshot classification can introduce bias. In addition, most semantically relevant demonstrations are not taken into account. As with did with RAG, we will use a vector representation of the example to retrieve the most relevant demonstrations.\n",
        "4. Constrained Decoding: Finally, we'll discovering the `outlines` library, which contains modules that are useful to do constrained decoding.\n",
        "\n",
        "At the end of each section (except section 0.), there's a question to answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YVY9aJOg9bU"
      },
      "source": [
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tINI3qJvIQIE"
      },
      "outputs": [],
      "source": [
        "!pip install transformers bitsandbytes accelerate datasets outlines scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOxlDh8hQZGK"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    GenerationConfig\n",
        ")\n",
        "\n",
        "import torch\n",
        "\n",
        "# Put your hugging face token here: https://huggingface.co/docs/hub/en/security-tokens\n",
        "# You need to fill the access form with your huggingface account on this link: https://huggingface.co/mistralai/Ministral-8B-Instruct-2410\n",
        "hf_token = \"\"\n",
        "llm_name = \"mistralai/Ministral-8B-Instruct-2410\"\n",
        "\n",
        "# We want to use 4bit quantization to save memory\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=False, load_in_4bit=True\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_name, padding_side=\"left\", token=hf_token)\n",
        "# Prevent some transformers specific issues.\n",
        "tokenizer.use_default_system_prompt = False\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load LLM.\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    llm_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map={\"\": 0}, # load all the model layers on GPU 0\n",
        "    torch_dtype=torch.bfloat16, # float precision\n",
        "    token=hf_token\n",
        ")\n",
        "# Set LLM on eval mode.\n",
        "llm.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a93dg8OfWM6V"
      },
      "outputs": [],
      "source": [
        "# Set up our generation configuration.\n",
        "# We set max_new_token to 128 to reduce computation time (we may also lose some accuracy).\n",
        "# We disable beamsearch to ensure reproducibility (we may lose some accuracy).\n",
        "generation_config = GenerationConfig(\n",
        "  max_new_tokens = 128,\n",
        "  do_sample=False,\n",
        "  eos_token_id=tokenizer.eos_token_id,\n",
        "  pad_token_id=tokenizer.pad_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0z6wXOAGNi-o"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "id2label = {0:\"sadness\", 1:\"joy\", 2:\"love\", 3:\"anger\", 4:\"fear\", 5:\"surprise\"}\n",
        "\n",
        "\n",
        "# Dataset: https://huggingface.co/datasets/dair-ai/emotion\n",
        "ds = load_dataset(\"dair-ai/emotion\", \"split\")\n",
        "examples = [{\"text\":ex[\"text\"], \"label\":id2label[ex[\"label\"]]}for ex in ds['test'].to_list()]\n",
        "random.shuffle(examples)\n",
        "\n",
        "# Split examples and keep only a few samples to have short computation time.\n",
        "test, train = examples[:100], examples[100:500]\n",
        "print(f\"Train len {len(train)}. Test len {len(test)}\")\n",
        "print(f\"First example of test:\\n{test[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbxicYCqXFoJ"
      },
      "source": [
        "## 1. Zero-shot Classification\n",
        "\n",
        "It's very similar to what you've done last time, so we're providing you with most of the code. The only thing you need to code yourself is the parse_answer function.\n",
        "- We adapted the recommended classification prompt from: https://docs.mistral.ai/guides/prompting_capabilities/\n",
        "- The purpose of this function is to return the first occurrence of a correct label (sadness, joy, love, anger, fear, surprise)\n",
        "- We want to return \"\" if no answer is found.\n",
        "- You can use regex or string functions.\n",
        "\n",
        "There is a cell below to test your code. The output should be:\n",
        "\n",
        "```\n",
        "##### Example 0 #####\n",
        "# You're an expert in sentiment analysis. Your task is to classify the sentence emotion after <<<>>> with one of the following predefined labels:\n",
        "\n",
        "sadness\n",
        "joy\n",
        "love\n",
        "anger\n",
        "fear\n",
        "surprise\n",
        "\n",
        "You will only respond with the category. Do not include the word \"Category\". Do not provide explanations or notes.\n",
        "\n",
        "<<<\n",
        "Sentence: i feel a strange gratitude for the hated israeli occupation of sinai that lasted from to for actually recognizing the importance of sinais history\n",
        "Label:\n",
        ">>>\n",
        "# sadness\n",
        "# sadness\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwNIAD_uT53U"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "zeroshot_prompt = \"\"\"\n",
        "You're an expert in sentiment analysis. Your task is to classify the sentence emotion after <<<>>> with one of the following predefined labels:\n",
        "\n",
        "sadness\n",
        "joy\n",
        "love\n",
        "anger\n",
        "fear\n",
        "surprise\n",
        "\n",
        "You will only respond with the label. Do not include the word \"Label\". Do not provide explanations or notes.\n",
        "\n",
        "<<<\n",
        "Sentence: {sentence}\n",
        "Label:\n",
        ">>>\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def generate(prompt, llm=llm, generation_config=generation_config):\n",
        "\n",
        "  # Create turns with the given prompt\n",
        "  turns = [\n",
        "    {'role':'user', 'content':prompt}\n",
        "  ]\n",
        "\n",
        "  # Tokenize turns.\n",
        "  input_ids = tokenizer.apply_chat_template(turns, return_tensors='pt').to('cuda')\n",
        "\n",
        "  # Ensure we don't use gradient to save memory space and computation time.\n",
        "  with torch.no_grad():\n",
        "    outputs = llm.generate(\n",
        "      input_ids,\n",
        "      generation_config\n",
        "    )\n",
        "\n",
        "  # Recover and decode answer.\n",
        "  answer_tokens = outputs[0, input_ids.shape[1]:-1]\n",
        "  return tokenizer.decode(answer_tokens).strip()\n",
        "\n",
        "\n",
        "def parse_answer(answer):\n",
        "  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1lWv-GfTBEH"
      },
      "outputs": [],
      "source": [
        "# Test your code\n",
        "\n",
        "example = test[0]\n",
        "\n",
        "prompt = zeroshot_prompt.format(sentence=example[\"text\"])\n",
        "answer = generate(prompt)\n",
        "prediction = parse_answer(answer)\n",
        "\n",
        "print(\"##### Example 0 #####\")\n",
        "print(f\"# {prompt}\")\n",
        "print(f\"# {answer}\")\n",
        "print(f\"# {prediction}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLcsV6Rys-tx"
      },
      "source": [
        "Now apply the fewshot prompt on the full test dataset. You need report:\n",
        "- Accuracy (recall: number of correct answers divided by number of samples)\n",
        "- Ratio of missing answer (i.e \".\" answer)\n",
        "\n",
        "It should take 3 to 5 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcTlvyIlr1G0"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for example in tqdm(test): # tqdm allow you to track the progression of your loop.\n",
        "  # TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnJCGcVQh7FU"
      },
      "source": [
        "Note: We always find an answer, because we've used a “well-formed” prompt and because Mistral is good at following this type of instruction. If you try with the Lama-3, some answers may be missing.\n",
        "\n",
        "**Question: Are we sure that all these answer are \"well-formed\" answer ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNnwc7gXeC23"
      },
      "source": [
        "## 2. Fewshot Classification - Random Retrieval:\n",
        "\n",
        "Now we have a working zeroshot solution. Our next next step is to use demonstrations. We will start be implementing a random few shot generation. You need to implement 3 functions:\n",
        "\n",
        "- format_demo, wich format a given example into a demonstration string\n",
        "- format_demos, wich format a given list of example into a demonstration string (try to use format_demo)\n",
        "- get_random_demo, wich return k random examples. (you should use random.choice. https://docs.python.org/3/library/random.html)\n",
        "\n",
        "\n",
        "There is a cell below to test your code. The output should be:\n",
        "```\n",
        "##### format_demo #####\n",
        "# Sentence: i feel a strange gratitude for the hated israeli occupation of sinai that lasted from to for actually recognizing the importance of sinais history\n",
        "Label: surprise.\n",
        "\n",
        "\n",
        "##### format_demos #####\n",
        "# Sentence: i feel a strange gratitude for the hated israeli occupation of sinai that lasted from to for actually recognizing the importance of sinais history\n",
        "Label: surprise.\n",
        "\n",
        "Sentence: im feeling optimistic to finish out these last two weeks strong and probably continue with what i have been doing\n",
        "Label: joy.\n",
        "\n",
        "Sentence: i feel complacent and satisfied\n",
        "Label: joy.\n",
        "\n",
        "Sentence: im the only one with all the feelings and emotions and thats just pathetic of me to do so\n",
        "Label: sadness.\n",
        "\n",
        "Sentence: i just sat there in my group feeling really depressed because my book just had to go missing at this time\n",
        "Label: sadness.\n",
        "\n",
        "\n",
        "##### Example 0 #####\n",
        "# You're an expert in sentiment analysis. Your task is to classify the sentence emotion after <<<>>> with one of the following predefined labels:\n",
        "\n",
        "sadness\n",
        "joy\n",
        "love\n",
        "anger\n",
        "fear\n",
        "surprise\n",
        "\n",
        "You will only respond with the label. Do not include the word \"Label\". Do not provide explanations or notes.\n",
        "\n",
        "####\n",
        "Here are some examples:\n",
        "\n",
        "Sentence: i feel inspired so many thing i want to write down\n",
        "Label: joy.\n",
        "\n",
        "Sentence: i feel like i should have some sort of rockstar razzle dazzle lifestyle but i would at least like to spend a third of my life doing something i feel is worthwhile\n",
        "Label: joy.\n",
        "\n",
        "Sentence: i continue to write this i feel more and more distraught\n",
        "Label: fear.\n",
        "\n",
        "Sentence: i feel that third situation pretty much sums up my feelings toward this title\n",
        "Label: joy.\n",
        "\n",
        "Sentence: i remember wanting to fit in so bad and feeling like no one liked me\n",
        "Label: love.\n",
        "####\n",
        "\n",
        "<<<\n",
        "Sentence: i feel a strange gratitude for the hated israeli occupation of sinai that lasted from to for actually recognizing the importance of sinais history\n",
        "Label:\n",
        ">>>\n",
        "# sadness\n",
        "# sadness\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bJZMJVeZm-G"
      },
      "outputs": [],
      "source": [
        "fewshot_prompt = \"\"\"\n",
        "You're an expert in sentiment analysis. Your task is to classify the sentence emotion after <<<>>> with one of the following predefined labels:\n",
        "\n",
        "sadness\n",
        "joy\n",
        "love\n",
        "anger\n",
        "fear\n",
        "surprise\n",
        "\n",
        "You will only respond with the label. Do not include the word \"Label\". Do not provide explanations or notes.\n",
        "\n",
        "####\n",
        "Here are some examples:\n",
        "\n",
        "{examples}\n",
        "####\n",
        "\n",
        "<<<\n",
        "Sentence: {sentence}\n",
        "Label:\n",
        ">>>\n",
        "\"\"\".strip()\n",
        "\n",
        "def format_demo(demo):\n",
        "  # TODO\n",
        "\n",
        "def format_demos(demos):\n",
        "  # TODO\n",
        "\n",
        "def get_random_demo(k, train=train):\n",
        "  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_vIFNkJgzVX"
      },
      "outputs": [],
      "source": [
        "# Test your code !\n",
        "\n",
        "\n",
        "print(\"##### format_demo #####\")\n",
        "print(f\"# {format_demo(test[0])}\")\n",
        "\n",
        "\n",
        "print(\"\\n\\n##### format_demos #####\")\n",
        "print(f\"# {format_demos(test[:5])}\")\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "example = test[0]\n",
        "demos = format_demos(get_random_demo(5))\n",
        "\n",
        "prompt = fewshot_prompt.format(examples=demos, sentence=example[\"text\"])\n",
        "answer = generate(prompt)\n",
        "prediction = parse_answer(answer)\n",
        "\n",
        "print(\"\\n\\n##### Example 0 #####\")\n",
        "\n",
        "print(f\"# {prompt}\")\n",
        "print(f\"# {answer}\")\n",
        "print(f\"# {prediction}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E1Hr-cVF3pp"
      },
      "source": [
        "Now apply the fewshot prompt on the full test dataset. You need report:\n",
        "- Accuracy (recall: number of correct answers divided by number of samples)\n",
        "- Report them for k=1 and k=5\n",
        "\n",
        "It should take 5 to 7 minutes to run.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEAbpeiLu9IW"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "for example in tqdm(test): # tqdm allow you to track the progression of your loop.\n",
        "  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_F1U0jMnDhf"
      },
      "source": [
        "**Question: What are the limits of using a single demonstration? What are the limits of using too many demonstrations?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvO1K4K_xstS"
      },
      "source": [
        "## 3. Fewshot Classification - Vector-based Retrieval\n",
        "\n",
        "Now, we want to improve demonstrayion by the vector representation of our sentence. This is close to what we did when we used RAG on wikipedia page. But here, we'll do it manually and step by step.\n",
        "\n",
        "To do so, we need to calculate the vector representation of our training dataset. To do this, we'll code a function that returns a vector for a given example. We'll use our LLM hidden states to do this. It's not optimal, but we won't have to load another model.\n",
        "\n",
        "First, look at the mistral architecture:\n",
        "\n",
        "```\n",
        "MistralForCausalLM(\n",
        "  (model): MistralModel(\n",
        "    (embed_tokens): Embedding(131072, 4096)\n",
        "    (layers): ModuleList(\n",
        "      (0-35): 36 x MistralDecoderLayer(\n",
        "        (self_attn): MistralSdpaAttention(\n",
        "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
        "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
        "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (rotary_emb): MistralRotaryEmbedding()\n",
        "        )\n",
        "        (mlp): MistralMLP(\n",
        "          (gate_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
        "          (up_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
        "          (down_proj): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
        "          (act_fn): SiLU()\n",
        "        )\n",
        "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
        "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
        "      )\n",
        "    )\n",
        "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
        "  )\n",
        "  (lm_head): Linear(in_features=4096, out_features=131072, bias=False)\n",
        ")\n",
        "```\n",
        "There are 36 transformer layers and 1 language model (LM) layer. Each layer will take the following shape: [1, N_TOKENS, N_PARAMS]. We want to extract the vector of the last token from the last transformer. To do so:\n",
        "- Encode the sentence without any template. `tokenizer.encode(...)`\n",
        "- Use the `output_hidden_states` keyword of the llm forward function.\n",
        "- Select the last transformer layer (be careful, don't take the LM layer).\n",
        "- Select the last token.\n",
        "- Convert the vector to numpy `.to('cpu').float().numpy()` and return it.\n",
        "\n",
        "There is a cell below to test your code. The output should be:\n",
        "```\n",
        "# (4096,)\n",
        "# [ 4.59375    -9.          0.80078125 ...  0.890625   -0.20019531\n",
        " -0.62109375]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOeTRJZSfOsR"
      },
      "outputs": [],
      "source": [
        "def get_hidden_repr(text, llm=llm, prompt_template=zeroshot_prompt):\n",
        "  # TODO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hekLonhPd-MA"
      },
      "outputs": [],
      "source": [
        "example = train[0]\n",
        "vector = get_hidden_repr(example[\"text\"])\n",
        "print(\"#\", vector.shape)\n",
        "print(\"#\", vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F_KmHorGn4B"
      },
      "source": [
        "Now, we need to get the hidden represation vector for all examples in the train and the test datasets.\n",
        "\n",
        "You should store the vector directly in the example dict: `example[\"vector\"] = ...`\n",
        "\n",
        "Both should take 3 - 5 mins to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdyPc5OZ0CkB"
      },
      "outputs": [],
      "source": [
        "for example in tqdm(train): # tqdm allow you to track the progression of your loop.\n",
        "  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J0hBUiS6fCp"
      },
      "outputs": [],
      "source": [
        "# Same for test examples !\n",
        "for example in tqdm(test): # tqdm allow you to track the progression of your loop.\n",
        "  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHyFAsmR6QkA"
      },
      "source": [
        "Now that we have our vector representations. We want a function that compute the cosine similarity between 2 examples.\n",
        "\n",
        "- Use the function from sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
        "- Be careful, you have to reshape each vector to: [1, 4096]\n",
        "\n",
        "There is a cell below to test your code. The output should be:\n",
        "```\n",
        "# a . a = 1.0000019073486328\n",
        "# a . b = 0.930396318435669\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpLMGStl6Of9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_similarity(example_a, example_b):\n",
        "  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5ytdMfp0GVr"
      },
      "outputs": [],
      "source": [
        "# Test your code !\n",
        "\n",
        "a, b = train[0], train[1]\n",
        "\n",
        "print(f\"# a . a = {compute_similarity(a, a)}\")\n",
        "print(f\"# a . b = {compute_similarity(a, b)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p4LQCDw9JSU"
      },
      "source": [
        "Last step, we want a function that retrieve the k more similar demonstrations of the train examples given a test example.\n",
        "\n",
        "There is a cell below to test your code. The output should be:\n",
        "```\n",
        "# surprise - i feel a strange gratitude for the hated israeli occupation of sinai that lasted from to for actually recognizing the importance of sinais history\n",
        "#  joy - i feel lucky that theyve chosen to share their lives with me\n",
        "\n",
        "joy - i feel our world then was a much more innocent place\n",
        "\n",
        "joy - i know he does the same thing for so many passersby i feel special truly welcome in his country\n",
        "\n",
        "joy - i do know that i tell some people if i feel that their question is sincere some of my sacred treasures\n",
        "\n",
        "anger - i feel appalled that i took advantage of my old friend s kindness\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDwXVZEc9Ieb"
      },
      "outputs": [],
      "source": [
        "def get_k_similar_demo(example, k, train=train):\n",
        "  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWb92RZq8sH5"
      },
      "outputs": [],
      "source": [
        "# Test your code !\n",
        "example = test[0]\n",
        "print(f\"# {example['label']} - {example['text']}\")\n",
        "print(\"# \", \"\\n\\n\".join([f\"{ex['label']} - {ex['text']}\" for ex in get_k_similar_demo(example, 5)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYh2wnruGdNh"
      },
      "source": [
        "Now apply the fewshot prompt on the full test dataset. You need report:\n",
        "- Accuracy (recall: number of correct answers divided by number of samples)\n",
        "- Report them for k=1 and k=5\n",
        "\n",
        "It should take 5 to 7 minutes to run.\n",
        "\n",
        "Your results should be:\n",
        "```\n",
        "##### k=1 #####\n",
        "Accuracy:  0.65\n",
        "##### k=5 #####\n",
        "Accuracy:  0.63\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d4UykmE9FqX"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for example in tqdm(test): # tqdm allow you to track the progression of your loop.\n",
        "  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBuOAMeypO1w"
      },
      "source": [
        "**Question: What could be the main issue with this approach? How can it be mitigated?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzotKcwQKy44"
      },
      "source": [
        "## 4. Constrained Decoding\n",
        "\n",
        "Last exercise, we will use the `outlines` package to do constrained generation. This main idea is to guide the generation of the LLM to get the good output formats.\n",
        "\n",
        "We will use the choices module. Here is the documentation: https://dottxt-ai.github.io/outlines/latest/reference/generation/choices/\n",
        "\n",
        "There is an example below on how to use it on 1 example. We let you apply this methods to the test dataset. You need report:\n",
        "- Accuracy (recall: number of correct answers divided by number of samples)\n",
        "- Ratio of missing answer (i.e \"E.\" answer)\n",
        "- Report them for k=1 and k=5\n",
        "\n",
        "It should take 3 to 5 minutes to run.\n",
        "\n",
        "Your results should be:\n",
        "```\n",
        "Accuracy:  0.38\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZTiTM04__PE"
      },
      "outputs": [],
      "source": [
        "from outlines import models, generate\n",
        "\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BEiQQm2p9aG"
      },
      "source": [
        "**Question: Now that you've used all these solutions, when should you use zeroshot? when should you use fewshot? when should you use constrained decoding?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H_HqjALp3pG"
      },
      "source": [
        "## Bonus\n",
        "\n",
        "Try to use differents modules of `outlines` like json, pydantic or regex ...\n",
        "\n",
        "Compare this results with previous ones !"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
