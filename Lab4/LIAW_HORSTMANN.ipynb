{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "    <img alt=\"Institut Polytechnique de Paris Logo\" width=\"auto\" height=\"150px\" src=\"https://www.ip-paris.fr/sites/default/files/presse/Charte%20Graphique/Logo%20IP%20Paris%206%20%C3%A9coles%20vertical%20png.png\" />\n",
        "</div>\n",
        "\n",
        "<div style=\"text-align: center; font-family: Arial, sans-serif; margin: 20px 0;\">\n",
        "    <h1 style=\"font-size: 32px; margin-bottom: 10px;\">Lab 3: TableGPT</h1>\n",
        "    <p style=\"font-size: 16px; margin: 0;\">Authors: \n",
        "        <strong>Tim Luka Horstmann</strong> & <strong>William Liaw</strong>\n",
        "    </p>\n",
        "    \n",
        "</div>\n",
        "\n",
        "---\n",
        "**Tim Luka Horstmann**\n",
        "\n",
        "**Email:** [tim.horstmann@ip-paris.fr](mailto:tim.horstmann@ip-paris.fr)  \n",
        "**Website:** [horstmann.tech](https://horstmann.tech)\n",
        "\n",
        "\n",
        "**William Liaw**\n",
        "\n",
        "**Email:** [william.liaw@telecom-paris.fr](mailto:tim.horstmann@ip-paris.fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOG3pXs8TxLY"
      },
      "source": [
        "In this lab, we'll discover the power of code generation models through TableGPT2. The aim is to see how the model can be used in data analysis.\n",
        "\n",
        "First of all, the notebook is divided into X sections:\n",
        "0. Installation: This section is dedicated to module installation, model loading and data loading.\n",
        "1. Guided introduction: Together, we'll discover how to use and evaluate TableGPT2.\n",
        "2. More questions: You'll need to add at least one new question type to our simple evaluation system.\n",
        "3. More data sets: You'll need to implement a question with multiple datasets.\n",
        "\n",
        "\n",
        "IMPORTANT:\n",
        "- You must work in pairs. You must submit **ONLY ONE NOTEBOOK** for each pair.\n",
        "- Do not share your work with other pairs.\n",
        "- You should not use Copilot, ChatGPT or similar tools. At the very least, remove the prompt ...\n",
        "- <font color='red'>All the things you need to do are indicated in red.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color='red'>**FIRST QUESTION:** What are the specificty of the TableGPT2 model?</font> https://huggingface.co/tablegpt/TableGPT2-7B\n",
        "\n",
        "<h2 style=\"\n",
        "    font-family: 'Times New Roman', Times, serif; \n",
        "    color: #cfcfcf; \n",
        "    font-size: 22px; \n",
        "    font-weight: normal; \n",
        "    border-bottom: 1px solid #ddd; \n",
        "    padding-bottom: 5px; \n",
        "    margin-bottom: 15px;\">\n",
        "    Answer\n",
        "</h2>\n",
        "\n",
        "The TableGPT2 model was developed to extend the capabilities of traditional pre-trained language models to structured data, specifically tables, addressing their limitations in understanding and processing tabular information.\n",
        "\n",
        "Unlike general-purpose language models, TableGPT2 was trained using instruction tuning, where training examples were structured as (instruction, table, completion) pairs. Many of these examples were annotated by human labelers to enhance performance. The model is built on the Qwen2.5 architecture and underwent continual pre-training on a diverse set of tables, optimizing it for structured data processing. A key innovation is its specialized table encoding, which improves interpretation across different table components, including rows, columns, and entire tables. The model also benefits from supervised fine-tuning to further enhance its effectiveness.\n",
        "\n",
        "TableGPT2 delivers strong performance across various table-related tasks, including entity matching, error detection, data imputation, table-based question answering, and more.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az4ycBgrXuDU"
      },
      "source": [
        "<div style=\"\n",
        "    background-color: #2C3E50; \n",
        "    color: #ECF0F1; \n",
        "    font-size: 28px; \n",
        "    text-align: center; \n",
        "    padding: 20px; \n",
        "    border-radius: 15px; \n",
        "    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2); \n",
        "    width: 100%; \n",
        "    margin: auto; \n",
        "    font-family: Times New Roman, sans-serif;\">\n",
        "    0. Setup\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MlTjE4nDH05G"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers datasets bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TYDBffMEX5Pw"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import (AutoModelForCausalLM, AutoTokenizer,\n",
        "                          BitsAndBytesConfig, GenerationConfig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU7OdU7mzyfN"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81b514b470c244a1b947d71397d38e1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(152064, 3584)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2SdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "          (rotary_emb): Qwen2RotaryEmbedding()\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_name = \"tablegpt/TableGPT2-7B\"\n",
        "\n",
        "# We want to use 4bit quantization to save memory\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=False, load_in_4bit=True\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_name, padding_side=\"left\", cache_dir=\"/Data/tlh45\")\n",
        "# Prevent some transformers specific issues.\n",
        "tokenizer.use_default_system_prompt = False\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load LLM.\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    llm_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map={\"\": 0}, # load all the model layers on GPU 0\n",
        "    torch_dtype=torch.bfloat16, # float precision\n",
        "    cache_dir=\"/Data/tlh45\"\n",
        ")\n",
        "# Set LLM on eval mode.\n",
        "llm.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HAciK5cIWxgb"
      },
      "outputs": [],
      "source": [
        "generation_config = GenerationConfig(\n",
        "  max_new_tokens=512,\n",
        "  do_sample=False,\n",
        "  # do_sample=True,\n",
        "  # temperature=.7,\n",
        "  # top_p=.8,\n",
        "  # top_k=20,\n",
        "  eos_token_id=tokenizer.eos_token_id,\n",
        "  pad_token_id=tokenizer.pad_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME-E7zK-zZwU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 712 entries, 0 to 890\n",
            "Data columns (total 11 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  712 non-null    int64  \n",
            " 1   Survived     712 non-null    int64  \n",
            " 2   Pclass       712 non-null    int64  \n",
            " 3   Name         712 non-null    object \n",
            " 4   Sex          712 non-null    object \n",
            " 5   Age          712 non-null    float64\n",
            " 6   SibSp        712 non-null    int64  \n",
            " 7   Parch        712 non-null    int64  \n",
            " 8   Ticket       712 non-null    object \n",
            " 9   Fare         712 non-null    float64\n",
            " 10  Embarked     712 non-null    object \n",
            "dtypes: float64(2), int64(5), object(4)\n",
            "memory usage: 66.8+ KB\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"hf://datasets/phihung/titanic/train.csv\")\n",
        "df = df.drop(\"Cabin\", axis=1).dropna()\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  \\\n",
              "0            1         0       3   \n",
              "1            2         1       1   \n",
              "2            3         1       3   \n",
              "3            4         1       1   \n",
              "4            5         0       3   \n",
              "\n",
              "                                                Name     Sex   Age  SibSp  \\\n",
              "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
              "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
              "4                           Allen, Mr. William Henry    male  35.0      0   \n",
              "\n",
              "   Parch            Ticket     Fare Embarked  \n",
              "0      0         A/5 21171   7.2500        S  \n",
              "1      0          PC 17599  71.2833        C  \n",
              "2      0  STON/O2. 3101282   7.9250        S  \n",
              "3      0            113803  53.1000        S  \n",
              "4      0            373450   8.0500        S  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"\n",
        "    background-color: #2C3E50; \n",
        "    color: #ECF0F1; \n",
        "    font-size: 28px; \n",
        "    text-align: center; \n",
        "    padding: 20px; \n",
        "    border-radius: 15px; \n",
        "    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2); \n",
        "    width: 100%; \n",
        "    margin: auto; \n",
        "    font-family: Times New Roman, sans-serif;\">\n",
        "    1.1 Guided Introduction: The Model.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPxFK_SbYIGf"
      },
      "source": [
        "Below there is an example of a prompt that could be used with TableGPT2.\n",
        "\n",
        "```\n",
        "Given access to several pandas dataframes, write the Python code to answer the user's question.\n",
        "The answer should be store in a variable named \"output\".\n",
        "\n",
        "/*\n",
        "\"df.head(5).to_string(index=False)\" as follows:\n",
        " PassengerId  Survived  Pclass                                                Name    Sex  Age  SibSp  Parch           Ticket    Fare Embarked\n",
        "           1         0       3                             Braund, Mr. Owen Harris   male 22.0      1      0        A/5 21171  7.2500        S\n",
        "           2         1       1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38.0      1      0         PC 17599 71.2833        C\n",
        "           3         1       3                              Heikkinen, Miss. Laina female 26.0      0      0 STON/O2. 3101282  7.9250        S\n",
        "           4         1       1        Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0      1      0           113803 53.1000        S\n",
        "           5         0       3                            Allen, Mr. William Henry   male 35.0      0      0           373450  8.0500        S\n",
        "*/\n",
        "\n",
        "Question: How many child survive? (under 18)\n",
        "```\n",
        "\n",
        "The prompt is divided in 3 parts:\n",
        "1. The global instruction wich is to write python that could answer a question on a specific dataset.\n",
        "2. The header of the given dataset: 5 first lines of titanic dataset.\n",
        "3. The question to answer: \"How many child survive? (under 18)\n",
        "\n",
        "\n",
        "First, we will implement a function that generate an answer for this prompt.\n",
        "\n",
        "<font color='red'>TODO: Fill in the `generate_answer` function following the comments inside.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvUClWNx1jEj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given access to several pandas dataframes, write the Python code to answer the user's question.\n",
            "The answer should be store in a variable named \"output\". Don't put print statements in the code.\n",
            "\n",
            "/*\n",
            "\"df.head(5).to_string(index=False)\" as follows:\n",
            " PassengerId  Survived  Pclass                                                Name    Sex  Age  SibSp  Parch           Ticket    Fare Embarked\n",
            "           1         0       3                             Braund, Mr. Owen Harris   male 22.0      1      0        A/5 21171  7.2500        S\n",
            "           2         1       1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38.0      1      0         PC 17599 71.2833        C\n",
            "           3         1       3                              Heikkinen, Miss. Laina female 26.0      0      0 STON/O2. 3101282  7.9250        S\n",
            "           4         1       1        Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0      1      0           113803 53.1000        S\n",
            "           5         0       3                            Allen, Mr. William Henry   male 35.0      0      0           373450  8.0500        S\n",
            "*/\n",
            "\n",
            "Question: How many child survive? (under 18)\n",
            "\n",
            "\n",
            "*****\n",
            "\n",
            "Python code:\n",
            "```python\n",
            "# Filter the dataframe to include only passengers under the age of 18\n",
            "children = df[df['Age'] < 18]\n",
            "\n",
            "# Count the number of children who survived\n",
            "child_survivors = children[children['Survived'] == 1]\n",
            "\n",
            "# Save the answer in the variable 'output'\n",
            "output = len(child_survivors)\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "example_prompt_template = \"\"\"Given access to several pandas dataframes, write the Python code to answer the user's question.\n",
        "The answer should be store in a variable named \"output\". Don't put print statements in the code.\n",
        "\n",
        "/*\n",
        "\"{var_name}.head(5).to_string(index=False)\" as follows:\n",
        "{df_info}\n",
        "*/\n",
        "\n",
        "Question: {user_question}\n",
        "\"\"\"\n",
        "\n",
        "def generate_answer(prompt, llm=llm, generation_config=generation_config):\n",
        "    # Create turns with the given prompt\n",
        "    turns = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # Tokenize turns.\n",
        "    input_ids = tokenizer.apply_chat_template(turns, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Ensure we don't use gradient to save memory space and computation time.\n",
        "    with torch.no_grad():\n",
        "        outputs = llm.generate(input_ids, generation_config)\n",
        "\n",
        "    # Recover and decode answer.\n",
        "    answer_tokens = outputs[0, input_ids.shape[1] : -1]\n",
        "    return tokenizer.decode(answer_tokens).strip()\n",
        "\n",
        "\n",
        "\n",
        "prompt = example_prompt_template.format(\n",
        "    var_name=\"df\",\n",
        "    df_info=df.head(5).to_string(index=False),\n",
        "    user_question=\"How many child survive? (under 18)\",\n",
        ")\n",
        "\n",
        "answer = generate_answer(prompt)\n",
        "\n",
        "print(prompt)\n",
        "print(\"\\n*****\\n\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nt90EerdwBN"
      },
      "source": [
        "<div style=\"\n",
        "    background-color: #2C3E50; \n",
        "    color: #ECF0F1; \n",
        "    font-size: 28px; \n",
        "    text-align: center; \n",
        "    padding: 20px; \n",
        "    border-radius: 15px; \n",
        "    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2); \n",
        "    width: 100%; \n",
        "    margin: auto; \n",
        "    font-family: Times New Roman, sans-serif;\">\n",
        "    1.2 Guided Introduction: The Answer.\n",
        "\n",
        "</div>\n",
        "\n",
        "As you can see, the model answer with some generated code.\n",
        "\n",
        "```\n",
        "Python code:\n",
        "```python\n",
        "# Filter the dataframe to include only passengers under the age of 18\n",
        "children = df[df['Age'] < 18]\n",
        "\n",
        "# Count the number of children who survived\n",
        "child_survivors = children[children['Survived'] == 1]\n",
        "\n",
        "# Save the answer in the variable output\n",
        "output = len(child_survivors)\n",
        "```\n",
        "\n",
        "So we will need to execute it, but there is some difficulty:\n",
        "1. Sometime, the llm answer with \\`\\`\\`python ... \\`\\`\\`, sometime the llm answer directly with the code. We need to handle both cases.\n",
        "2. We need to recover the variable output from the execution.\n",
        "3. We need to evaluate single value and list of values.\n",
        "\n",
        "\n",
        "First, we will implement a function that generate an answer for this prompt.\n",
        "\n",
        "<font color='red'>TODO: Fill in the `exec_answer` function following the comments inside.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "z_hwvxSGMSlo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "def exec_answer(answer, gold):\n",
        "\n",
        "    # Extract the code from the answer. Be careful, the code is now always in ``` ```.\n",
        "    code_fence_pattern = re.compile(r\"```python\\s*(.*?)\\s*```\", re.DOTALL | re.IGNORECASE)\n",
        "    match = code_fence_pattern.search(answer)\n",
        "\n",
        "    if match:\n",
        "        code = match.group(1)\n",
        "    else:\n",
        "        code = answer.strip()\n",
        "\n",
        "    # Execute the code, https://docs.python.org/3/library/functions.html#exec\n",
        "    try:\n",
        "        local_namespace = {}\n",
        "        exec(code, globals(), local_namespace)\n",
        "\n",
        "        if \"output\" not in local_namespace:\n",
        "            raise ValueError(\"The code did not define the variable `output`.\")\n",
        "\n",
        "        output = local_namespace[\"output\"]\n",
        "\n",
        "        # if the code work: Return True or False based on output == gold (be careful to handle iterable !)\n",
        "        if isinstance(output, pd.Series) or isinstance(output, np.ndarray):\n",
        "            output = output.tolist()\n",
        "        if isinstance(gold, pd.Series) or isinstance(gold, np.ndarray):\n",
        "            gold = gold.tolist()\n",
        "        if isinstance(output, (list, tuple)) and isinstance(gold, (list, tuple)):\n",
        "            return (list(output) == list(gold))\n",
        "        else:\n",
        "            return output == gold\n",
        "\n",
        "    except Exception as e:\n",
        "        # if the code don't work return False.\n",
        "        print(f\"Error during execution: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "print(exec_answer(answer, 61))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1TrmGcQIEeI"
      },
      "source": [
        "<div style=\"\n",
        "    background-color: #2C3E50; \n",
        "    color: #ECF0F1; \n",
        "    font-size: 28px; \n",
        "    text-align: center; \n",
        "    padding: 20px; \n",
        "    border-radius: 15px; \n",
        "    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2); \n",
        "    width: 100%; \n",
        "    margin: auto; \n",
        "    font-family: Times New Roman, sans-serif;\">\n",
        "    1.3 Guided Introduction: The Question.\n",
        "\n",
        "</div>\n",
        "\n",
        "Now we want to automatically generate questions to evaluate the performance of our model. There are benchmarks on this subject, but here we want to practice code by generating the questions ourselves.\n",
        "\n",
        "We will generate some basic filter questions.\n",
        "\n",
        "<font color='red'>TODO: Fill in the `generate_filter_question` function following the comments inside.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'question': 'Which entries in `Embarked` correspond to rows where `Survived` = `0`?', 'answer': ['S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'Q', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'Q', 'Q', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'C', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'Q', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'Q', 'S', 'C', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'C', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'Q']}, {'question': 'Which entries in `Sex` correspond to rows where `Name` = `Stankovic, Mr. Ivan`?', 'answer': ['male']}, {'question': 'Which entries in `Pclass` correspond to rows where `Survived` = `0`?', 'answer': [3, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 1, 1, 2, 1, 1, 3, 3, 3, 2, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 1, 2, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 2, 1, 3, 2, 2, 1, 3, 3, 3, 3, 2, 2, 1, 3, 1, 3, 2, 2, 3, 2, 2, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 1, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 1, 3, 2, 3, 1, 3, 3, 1, 1, 2, 3, 1, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 3, 2, 2, 3, 2, 2, 3, 3, 3, 1, 1, 3, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 1, 3, 1, 2, 3, 3, 2, 1, 1, 3, 1, 2, 3, 1, 3, 3, 2, 3, 3, 3, 3, 1, 3, 3, 1, 1, 3, 1, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 2, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 3, 2, 3, 3, 3, 3, 3, 2, 1, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 2, 1, 3, 1, 3, 2, 2, 3, 1, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 1, 2, 3, 1, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 1, 3, 3, 3, 1, 2, 3, 2, 3, 3, 2, 3, 1, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 1, 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3]}, {'question': 'Which entries in `Fare` correspond to rows where `SibSp` = `1`?', 'answer': [7.25, 71.2833, 53.1, 30.0708, 16.7, 31.275, 18.0, 31.3875, 82.1708, 52.0, 11.2417, 9.475, 21.0, 41.5792, 17.8, 76.7292, 26.0, 27.75, 83.475, 14.4542, 34.375, 61.175, 20.575, 26.0, 14.4542, 9.825, 21.0, 30.0708, 11.2417, 14.5, 26.0, 53.1, 15.85, 36.75, 66.6, 27.9, 11.1333, 7.8542, 15.5, 7.8542, 26.0, 15.85, 113.275, 27.0, 90.0, 83.475, 26.0, 52.5542, 26.0, 10.4625, 16.1, 79.65, 7.775, 77.9583, 20.25, 91.0792, 151.55, 151.55, 108.9, 24.0, 26.0, 26.25, 26.0, 134.5, 29.0, 20.525, 66.6, 26.0, 15.9, 7.2292, 17.8, 27.9, 27.7208, 75.25, 55.4417, 6.4958, 52.0, 120.0, 113.275, 9.825, 15.85, 21.0, 18.75, 90.0, 32.5, 14.4, 20.2125, 26.0, 26.0, 55.9, 120.0, 263.0, 26.25, 7.775, 27.75, 89.1042, 27.75, 21.0, 7.0458, 91.0792, 90.0, 15.9, 78.2667, 151.55, 108.9, 59.4, 26.0, 26.0, 7.2292, 26.0, 106.425, 26.0, 20.525, 36.75, 39.6, 79.65, 17.4, 55.9, 30.0, 110.8833, 79.2, 78.2667, 26.0, 24.15, 56.9292, 15.55, 41.5792, 31.275, 65.0, 14.4, 16.1, 14.4542, 52.5542, 15.7417, 26.25, 76.7292, 15.5, 7.925, 39.0, 52.0, 46.9, 39.0, 41.5792, 57.0, 110.8833, 227.525, 7.8542, 52.0, 7.0542, 53.1, 26.0, 7.925, 34.375, 78.85, 16.1, 71.0, 20.25, 53.1, 23.0, 65.0, 14.5, 120.0, 77.9583, 23.0, 57.0, 20.575, 24.15, 26.25, 120.0, 53.1, 37.0042, 93.5, 14.4542, 18.75, 83.1583, 15.2458, 26.0, 164.8667, 11.5, 13.8583, 11.1333, 52.5542, 24.0]}, {'question': 'Which entries in `PassengerId` correspond to rows where `Survived` = `0`?', 'answer': [1, 5, 7, 8, 13, 14, 15, 17, 19, 21, 25, 28, 31, 34, 35, 36, 38, 39, 41, 42, 50, 51, 52, 55, 58, 60, 61, 63, 64, 68, 70, 71, 72, 73, 74, 76, 81, 84, 87, 90, 91, 92, 93, 94, 95, 97, 100, 101, 103, 104, 105, 106, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 125, 130, 131, 132, 133, 135, 136, 138, 139, 140, 144, 145, 146, 148, 149, 150, 151, 153, 154, 156, 158, 161, 163, 164, 165, 168, 170, 171, 172, 174, 175, 176, 178, 179, 180, 183, 189, 190, 192, 198, 200, 201, 203, 204, 206, 207, 211, 213, 214, 218, 220, 222, 223, 226, 228, 229, 232, 233, 235, 237, 239, 240, 243, 244, 245, 246, 247, 250, 252, 253, 254, 255, 263, 264, 266, 267, 274, 277, 279, 281, 282, 283, 286, 288, 293, 294, 295, 297, 298, 303, 309, 313, 314, 315, 318, 321, 322, 327, 332, 333, 334, 337, 340, 343, 344, 345, 350, 351, 353, 354, 356, 358, 361, 362, 363, 364, 366, 372, 373, 374, 375, 378, 379, 380, 383, 386, 387, 393, 396, 397, 398, 399, 402, 403, 404, 405, 406, 407, 409, 419, 420, 422, 423, 424, 425, 434, 435, 437, 439, 440, 442, 443, 451, 453, 457, 462, 463, 464, 466, 468, 472, 475, 477, 478, 479, 481, 483, 488, 489, 492, 493, 494, 495, 499, 500, 501, 502, 504, 506, 509, 515, 516, 520, 522, 526, 529, 530, 533, 535, 537, 542, 543, 545, 546, 549, 552, 556, 562, 563, 566, 567, 568, 575, 576, 583, 584, 587, 589, 591, 593, 595, 596, 598, 604, 606, 607, 611, 615, 617, 618, 620, 621, 624, 625, 626, 627, 629, 632, 635, 637, 638, 639, 641, 643, 647, 653, 655, 656, 658, 659, 660, 662, 663, 664, 666, 667, 669, 672, 673, 676, 677, 679, 683, 684, 685, 686, 687, 688, 689, 694, 695, 696, 697, 699, 700, 703, 704, 705, 706, 714, 715, 716, 720, 722, 723, 724, 726, 729, 730, 732, 734, 735, 736, 737, 742, 744, 746, 747, 749, 750, 753, 754, 757, 758, 759, 762, 765, 768, 770, 771, 772, 773, 776, 783, 785, 786, 788, 790, 792, 795, 796, 799, 800, 801, 806, 807, 808, 809, 811, 812, 813, 814, 815, 817, 818, 819, 820, 823, 825, 834, 835, 837, 841, 842, 844, 845, 846, 848, 849, 851, 852, 853, 855, 861, 862, 865, 868, 871, 873, 874, 877, 878, 882, 883, 884, 885, 886, 887, 891]}]\n"
          ]
        }
      ],
      "source": [
        "def generate_random_question(generate_function, df, k=1, seed=42):\n",
        "    random.seed(seed)\n",
        "    return [generate_function(df) for _ in range(k)]\n",
        "\n",
        "\n",
        "def generate_filter_question(df):\n",
        "    # Create a question template that take a target colunm, a filter column and a filter value\n",
        "    question_template = \"Which entries in `{target_column}` correspond to rows where `{filter_column}` = `{filter_value}`?\"\n",
        "\n",
        "    # Get a random target column and a random filter column (be careful they should be differnts)\n",
        "    columns = list(df.columns)\n",
        "\n",
        "    if len(columns) < 2:\n",
        "        raise ValueError(\n",
        "            \"DataFrame must have at least two columns to create a valid question.\"\n",
        "        )\n",
        "\n",
        "    # Ensure the target column and filter column are different\n",
        "    target_column, filter_column = random.sample(columns, 2)\n",
        "\n",
        "    # Get a random filter value inside the filer column. Avoid NaN values.\n",
        "    valid_values = df[filter_column].dropna().unique()\n",
        "\n",
        "    if len(valid_values) == 0:\n",
        "        raise ValueError(\n",
        "            f\"No valid values found in column {filter_column} for filtering.\"\n",
        "        )\n",
        "\n",
        "    filter_value = random.choice(valid_values)\n",
        "\n",
        "    # Compute the correct answer for the given target column, filter column and filter value.\n",
        "    filtered_df = df[df[filter_column] == filter_value]\n",
        "    answer = filtered_df[target_column].tolist()\n",
        "\n",
        "    # return formated question and associated answer in a dict {\"question\":[question], \"answer\":[answer]}\n",
        "    question = question_template.format(\n",
        "        target_column=target_column,\n",
        "        filter_column=filter_column,\n",
        "        filter_value=filter_value,\n",
        "    )\n",
        "\n",
        "    return {\"question\": question, \"answer\": answer}\n",
        "\n",
        "\n",
        "print(generate_random_question(generate_filter_question, df, k=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzDJ9c3VhHt2"
      },
      "source": [
        "<div style=\"\n",
        "    background-color: #2C3E50; \n",
        "    color: #ECF0F1; \n",
        "    font-size: 28px; \n",
        "    text-align: center; \n",
        "    padding: 20px; \n",
        "    border-radius: 15px; \n",
        "    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2); \n",
        "    width: 100%; \n",
        "    margin: auto; \n",
        "    font-family: Times New Roman, sans-serif;\">\n",
        "    1.4 Guided Introduction: The Evaluation.\n",
        "\n",
        "</div>\n",
        "\n",
        "The last step in this section is to evaluate our model on 20 random questions! We'll use simple accuracy.\n",
        "\n",
        "You should have an accuracy between 0.9 and 1.\n",
        "\n",
        "<font color='red'>TODO: Follow instruction in comment of the cell below.</font>\n",
        "\n",
        "<font color='green'>BONUS: Investigate on errors and improve our prompt/parsing to solve them.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiE53oszRGcq"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1053a57339bb4dd29407934ae2991c4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating model:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['male']\n",
            "71     5\n",
            "86     1\n",
            "138    0\n",
            "156    0\n",
            "208    0\n",
            "220    0\n",
            "266    4\n",
            "282    0\n",
            "329    0\n",
            "333    2\n",
            "504    0\n",
            "574    0\n",
            "746    1\n",
            "764    0\n",
            "791    0\n",
            "841    0\n",
            "853    0\n",
            "Name: SibSp, dtype: int64\n",
            "Acc:  1.0\n"
          ]
        }
      ],
      "source": [
        "# Generate 20 random question\n",
        "num_questions = 20\n",
        "questions = generate_random_question(generate_filter_question, df, k=num_questions)\n",
        "\n",
        "# Iterate over question to format prompt, generate answer and execute answer.\n",
        "correct = 0\n",
        "for entry in tqdm(questions, desc=\"Evaluating model\"):\n",
        "    question = entry[\"question\"]\n",
        "    gold_answer = entry[\"answer\"]  # The expected correct answer\n",
        "\n",
        "    # Generate the answer from the model\n",
        "    prompt = example_prompt_template.format(\n",
        "        var_name=\"df\",\n",
        "        df_info=df.head(5).to_string(index=False),\n",
        "        user_question=question,\n",
        "    )\n",
        "\n",
        "    generated_answer = generate_answer(prompt)\n",
        "\n",
        "    # Execute the generated answer and check if it's correct\n",
        "    if exec_answer(generated_answer, gold_answer):\n",
        "        correct += 1\n",
        "\n",
        "# Report the Accuracy\n",
        "accuracy = correct / num_questions\n",
        "\n",
        "print(\"Acc: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGYL5LUeiEBI"
      },
      "source": [
        "<div style=\"\n",
        "    background-color: #2C3E50; \n",
        "    color: #ECF0F1; \n",
        "    font-size: 28px; \n",
        "    text-align: center; \n",
        "    padding: 20px; \n",
        "    border-radius: 15px; \n",
        "    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2); \n",
        "    width: 100%; \n",
        "    margin: auto; \n",
        "    font-family: Times New Roman, sans-serif;\">\n",
        "    2. More Questions.\n",
        "\n",
        "</div>\n",
        "\n",
        "Now it's your turn to imagine a type of question (\"How many ...\"). Implement a function to generate new type of question. Verify that our previous code work with your new question then evaluate it.\n",
        "\n",
        "<font color='red'>TODO: Generate **AT LEAST ONE** new type of question and report this new question accuracy.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'question': 'How many rows have `Embarked` = `S`?', 'answer': 554}, {'question': 'How many rows have `Sex` = `male`?', 'answer': 453}, {'question': 'How many rows have `Pclass` > `3.0`?', 'answer': 0}, {'question': 'How many rows have `Embarked` = `Q`?', 'answer': 28}, {'question': 'How many rows have `SibSp` = `1.0`?', 'answer': 183}]\n"
          ]
        }
      ],
      "source": [
        "def generate_count_question(df):\n",
        "    # Question template\n",
        "    question_template = (\n",
        "        \"How many rows have `{filter_column}` {operator} `{filter_value}`?\"\n",
        "    )\n",
        "\n",
        "    columns = list(df.columns)\n",
        "    if len(columns) == 0:\n",
        "        raise ValueError(\n",
        "            \"DataFrame must have at least one column to create a question.\"\n",
        "        )\n",
        "\n",
        "    # Randomly pick a column to filter\n",
        "    filter_column = random.choice(columns)\n",
        "    # Identify if the column is numeric\n",
        "    col_dtype = df[filter_column].dtype\n",
        "\n",
        "    # If numeric, pick from {=, <, >}; otherwise only '='\n",
        "    if np.issubdtype(col_dtype, np.number):\n",
        "        possible_operators = [\"=\", \"<\", \">\"]\n",
        "    else:\n",
        "        possible_operators = [\"=\"]\n",
        "\n",
        "    operator = random.choice(possible_operators)\n",
        "\n",
        "    # Get valid (non-NaN) values\n",
        "    valid_values = df[filter_column].dropna().unique()\n",
        "    if len(valid_values) == 0:\n",
        "        raise ValueError(\n",
        "            f\"No valid (non-NaN) values found in column '{filter_column}' for filtering.\"\n",
        "        )\n",
        "\n",
        "    # Pick a random value for the filter\n",
        "    filter_value = random.choice(valid_values)\n",
        "\n",
        "    # Convert filter_value to a numeric type if the column is numeric\n",
        "    # (to avoid issues when comparing strings with < or >)\n",
        "    if np.issubdtype(col_dtype, np.number):\n",
        "        filter_value = float(filter_value)  # ensures numeric comparison is valid\n",
        "\n",
        "    # Apply the chosen operator for filtering\n",
        "    if operator == \"=\":\n",
        "        filtered_df = df[df[filter_column] == filter_value]\n",
        "    elif operator == \"<\":\n",
        "        filtered_df = df[df[filter_column] < filter_value]\n",
        "    else:\n",
        "        filtered_df = df[df[filter_column] > filter_value]\n",
        "\n",
        "    # Count the filtered rows\n",
        "    answer = len(filtered_df)\n",
        "\n",
        "    # Format the question\n",
        "    question = question_template.format(\n",
        "        filter_column=filter_column, operator=operator, filter_value=filter_value\n",
        "    )\n",
        "\n",
        "    return {\"question\": question, \"answer\": answer}\n",
        "\n",
        "\n",
        "print(generate_random_question(generate_count_question, df, k=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7273c1c81b949d3b42a2318144a77cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating model:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc:  1.0\n"
          ]
        }
      ],
      "source": [
        "# Generate 20 random question\n",
        "num_questions = 20\n",
        "questions = generate_random_question(generate_count_question, df, k=num_questions)\n",
        "\n",
        "# Iterate over question to format prompt, generate answer and execute answer.\n",
        "correct = 0\n",
        "for entry in tqdm(questions, desc=\"Evaluating model\"):\n",
        "    question = entry[\"question\"]\n",
        "    gold_answer = entry[\"answer\"]  # The expected correct answer\n",
        "\n",
        "    # Generate the answer from the model\n",
        "    prompt = example_prompt_template.format(\n",
        "        var_name=\"df\",\n",
        "        df_info=df.head(5).to_string(index=False),\n",
        "        user_question=question,\n",
        "    )\n",
        "\n",
        "    generated_answer = generate_answer(prompt)\n",
        "\n",
        "    # Execute the generated answer and check if it's correct\n",
        "    if exec_answer(generated_answer, gold_answer):\n",
        "        correct += 1\n",
        "\n",
        "# Report the Accuracy\n",
        "accuracy = correct / num_questions\n",
        "\n",
        "print(\"Acc: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt35rcuMoAdW"
      },
      "source": [
        "<div style=\"\n",
        "    background-color: #2C3E50; \n",
        "    color: #ECF0F1; \n",
        "    font-size: 28px; \n",
        "    text-align: center; \n",
        "    padding: 20px; \n",
        "    border-radius: 15px; \n",
        "    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2); \n",
        "    width: 100%; \n",
        "    margin: auto; \n",
        "    font-family: Times New Roman, sans-serif;\">\n",
        "    3. More datasets.\n",
        "\n",
        "</div>\n",
        "\n",
        "Below we load a new dataset: \"adult_income_dataset\".\n",
        "\n",
        "<font color='red'>TODO: Evaluate our questions on this new dataset. Report the accuracy. Comment Any differences.</font>\n",
        "\n",
        "<font color='green'>BONUS: Try to find a prompt that answer this question: What is the mean salary of titanic surviror based on adult dataset.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z33vyj_Sk_WX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 48842 entries, 0 to 48841\n",
            "Data columns (total 15 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   age              48842 non-null  int64 \n",
            " 1   workclass        48842 non-null  object\n",
            " 2   fnlwgt           48842 non-null  int64 \n",
            " 3   education        48842 non-null  object\n",
            " 4   educational-num  48842 non-null  int64 \n",
            " 5   marital-status   48842 non-null  object\n",
            " 6   occupation       48842 non-null  object\n",
            " 7   relationship     48842 non-null  object\n",
            " 8   race             48842 non-null  object\n",
            " 9   gender           48842 non-null  object\n",
            " 10  capital-gain     48842 non-null  int64 \n",
            " 11  capital-loss     48842 non-null  int64 \n",
            " 12  hours-per-week   48842 non-null  int64 \n",
            " 13  native-country   48842 non-null  object\n",
            " 14  income           48842 non-null  object\n",
            "dtypes: int64(6), object(9)\n",
            "memory usage: 5.6+ MB\n"
          ]
        }
      ],
      "source": [
        "adult = pd.read_csv(\"hf://datasets/meghana/adult_income_dataset/adult.csv\")\n",
        "adult.info()\n",
        "\n",
        "titanic = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  \\\n",
              "0            1         0       3   \n",
              "1            2         1       1   \n",
              "2            3         1       3   \n",
              "3            4         1       1   \n",
              "4            5         0       3   \n",
              "\n",
              "                                                Name     Sex   Age  SibSp  \\\n",
              "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
              "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
              "4                           Allen, Mr. William Henry    male  35.0      0   \n",
              "\n",
              "   Parch            Ticket     Fare Embarked  \n",
              "0      0         A/5 21171   7.2500        S  \n",
              "1      0          PC 17599  71.2833        C  \n",
              "2      0  STON/O2. 3101282   7.9250        S  \n",
              "3      0            113803  53.1000        S  \n",
              "4      0            373450   8.0500        S  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "titanic.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>educational-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>gender</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25</td>\n",
              "      <td>Private</td>\n",
              "      <td>226802</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>89814</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Farming-fishing</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28</td>\n",
              "      <td>Local-gov</td>\n",
              "      <td>336951</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Protective-serv</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>44</td>\n",
              "      <td>Private</td>\n",
              "      <td>160323</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>7688</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18</td>\n",
              "      <td>?</td>\n",
              "      <td>103497</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>?</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
              "0   25    Private  226802          11th                7       Never-married   \n",
              "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
              "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
              "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
              "4   18          ?  103497  Some-college               10       Never-married   \n",
              "\n",
              "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
              "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
              "1    Farming-fishing      Husband  White    Male             0             0   \n",
              "2    Protective-serv      Husband  White    Male             0             0   \n",
              "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
              "4                  ?    Own-child  White  Female             0             0   \n",
              "\n",
              "   hours-per-week native-country income  \n",
              "0              40  United-States  <=50K  \n",
              "1              50  United-States  <=50K  \n",
              "2              40  United-States   >50K  \n",
              "3              40  United-States   >50K  \n",
              "4              30  United-States  <=50K  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "adult.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<=50K' '>50K']\n"
          ]
        }
      ],
      "source": [
        "# check values in the column income in adult\n",
        "print(adult[\"income\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color='red'>TODO: Evaluate our questions on this new dataset. Report the accuracy. Comment Any differences.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d38415546034988935aca3bdaf35a1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating model:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'Black', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Asian-Pac-Islander', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'Black', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'Other', 'White', 'Black', 'White', 'Asian-Pac-Islander', 'Black', 'Black', 'Black', 'White', 'Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Amer-Indian-Eskimo', 'White', 'White', 'Black', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Other', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Asian-Pac-Islander', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'Black', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'Black', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'Black', 'Black', 'Black', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'Asian-Pac-Islander', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Black', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'Black', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'Black', 'White', 'Other', 'White', 'Black', 'White', 'Asian-Pac-Islander', 'White', 'Black', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Black', 'White', 'Other', 'White', 'White', 'Black', 'White', 'White', 'Other', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'Asian-Pac-Islander', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Other', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'White', 'Black', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'Other', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'Asian-Pac-Islander', 'White', 'Amer-Indian-Eskimo', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'Black', 'White', 'Black', 'Asian-Pac-Islander', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Amer-Indian-Eskimo', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Asian-Pac-Islander', 'White', 'Black', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'Asian-Pac-Islander', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'Black', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'Black', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Amer-Indian-Eskimo', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'Black', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'Black', 'White', 'Black', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'Amer-Indian-Eskimo', 'Black', 'Black', 'White', 'Black', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'Asian-Pac-Islander', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Other', 'White', 'White', 'Black', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'Black', 'Asian-Pac-Islander', 'White', 'Other', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'Black', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'Other', 'White', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'Asian-Pac-Islander', 'White', 'Black', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'White', 'Black', 'White', 'Black', 'Black', 'White', 'Black', 'White', 'White', 'Asian-Pac-Islander', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'Amer-Indian-Eskimo', 'White', 'Black', 'Other', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'Black', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'Black', 'Black', 'White', 'White', 'Other', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'Amer-Indian-Eskimo', 'White', 'White', 'Asian-Pac-Islander', 'White', 'White', 'Other', 'White', 'Black', 'Asian-Pac-Islander', 'White', 'Black', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'White', 'White', 'White', 'Black', 'Black', 'White', 'White', 'White', 'Asian-Pac-Islander', 'White', 'Black', 'White']\n",
            "1                 Private\n",
            "42                Private\n",
            "44              State-gov\n",
            "97                Private\n",
            "139               Private\n",
            "               ...       \n",
            "48525    Self-emp-not-inc\n",
            "48527    Self-emp-not-inc\n",
            "48534    Self-emp-not-inc\n",
            "48615             Private\n",
            "48711    Self-emp-not-inc\n",
            "Name: workclass, Length: 1490, dtype: object\n",
            "Acc:  1.0\n"
          ]
        }
      ],
      "source": [
        "# Generate 20 random question\n",
        "num_questions = 20\n",
        "questions = generate_random_question(generate_filter_question, adult, k=num_questions)\n",
        "\n",
        "# Iterate over question to format prompt, generate answer and execute answer.\n",
        "correct = 0\n",
        "for entry in tqdm(questions, desc=\"Evaluating model\"):\n",
        "    question = entry[\"question\"]\n",
        "    gold_answer = entry[\"answer\"]  # The expected correct answer\n",
        "\n",
        "    # Generate the answer from the model\n",
        "\n",
        "    prompt = example_prompt_template.format(\n",
        "        var_name=\"adult\",\n",
        "        df_info=adult.head(5).to_string(index=False),\n",
        "        user_question=question,\n",
        "    )\n",
        "\n",
        "    generated_answer = generate_answer(prompt)\n",
        "\n",
        "    # Execute the generated answer and check if it's correct\n",
        "    if exec_answer(generated_answer, gold_answer):\n",
        "        correct += 1\n",
        "\n",
        "# Report the Accuracy\n",
        "accuracy = correct / num_questions\n",
        "\n",
        "print(\"Acc: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11eaea30993b41e4b1b017ca52b0f17c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating model:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error during execution: The code did not define the variable `output`.\n",
            "Error during execution: The code did not define the variable `output`.\n",
            "Acc:  0.9\n"
          ]
        }
      ],
      "source": [
        "# Generate 20 random question\n",
        "num_questions = 20\n",
        "questions = generate_random_question(generate_count_question, adult, k=num_questions)\n",
        "\n",
        "# Iterate over question to format prompt, generate answer and execute answer.\n",
        "correct = 0\n",
        "for entry in tqdm(questions, desc=\"Evaluating model\"):\n",
        "    question = entry[\"question\"]\n",
        "    gold_answer = entry[\"answer\"]  # The expected correct answer\n",
        "\n",
        "    # Generate the answer from the model\n",
        "\n",
        "    prompt = example_prompt_template.format(\n",
        "        var_name=\"adult\",\n",
        "        df_info=adult.head(5).to_string(index=False),\n",
        "        user_question=question,\n",
        "    )\n",
        "\n",
        "    generated_answer = generate_answer(prompt)\n",
        "\n",
        "    # Execute the generated answer and check if it's correct\n",
        "    if exec_answer(generated_answer, gold_answer):\n",
        "        correct += 1\n",
        "\n",
        "# Report the Accuracy\n",
        "accuracy = correct / num_questions\n",
        "\n",
        "print(\"Acc: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color='red'>Comments on the differences</font>\n",
        "\n",
        "<h2 style=\"\n",
        "    font-family: 'Times New Roman', Times, serif; \n",
        "    color: #cfcfcf; \n",
        "    font-size: 22px; \n",
        "    font-weight: normal; \n",
        "    border-bottom: 1px solid #ddd; \n",
        "    padding-bottom: 5px; \n",
        "    margin-bottom: 15px;\">\n",
        "    Answer\n",
        "</h2>\n",
        "\n",
        "We observed a perfect (1.0) accuracy for filter questions on both the Titanic and Adult datasets. For count questions, the accuracy remained perfect (1.0) on the Titanic data but dropped slightly to 0.9 on the Adult data. This minor discrepancy may be due to the Adult dataset’s greater variety of categorical values and the occasional presence of placeholders (such as “?”). These factors may introduce hardships on filtering and counting scenarios for the LLM's interpretation and reasoning. Overall, the results still demonstrate robust performance for both question types and datasets, highlighting their effectiveness in extracting information from structured data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color='green'>BONUS: Try to find a prompt that answer this question: What is the mean salary of titanic surviror based on adult dataset.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: We decided to use the columns age and gender/sex as the merge/join basis for this task as it makes sense to compare salary based on these attributes, given the constraints of the dataset.\n",
        "Furthermore, we note that the salray (=income) is only given as ['<=50K' '>50K'] (check print(adult[\"income\"].unique()) above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated mean salary of Titanic survivors (based on adult dataset): 0.1632504674191538\n"
          ]
        }
      ],
      "source": [
        "# GET GOLD ANSWER\n",
        "\n",
        "# Work on copies to preserve original data\n",
        "titanic_copy = titanic.copy()\n",
        "adult_copy = adult.copy()\n",
        "\n",
        "titanic_copy.rename(columns=str.lower, inplace=True)\n",
        "adult_copy.rename(columns=str.lower, inplace=True)\n",
        "\n",
        "# 'gender' column in adult dataset to 'sex'\n",
        "if \"gender\" in adult_copy.columns:\n",
        "    adult_copy = adult_copy.rename(columns={\"gender\": \"sex\"})\n",
        "\n",
        "# to lowercase for consistency\n",
        "titanic_copy[\"sex\"] = titanic_copy[\"sex\"].str.lower()\n",
        "adult_copy[\"sex\"] = adult_copy[\"sex\"].str.lower()\n",
        "\n",
        "if \"age\" in adult_copy.columns and \"age\" in titanic_copy.columns:\n",
        "    adult_copy[\"age\"] = adult_copy[\"age\"].astype(float)\n",
        "    titanic_copy[\"age\"] = titanic_copy[\"age\"].astype(float)\n",
        "\n",
        "    # ages to nearest integer\n",
        "    adult_copy[\"age\"] = adult_copy[\"age\"].round().astype(int)\n",
        "    titanic_copy[\"age\"] = titanic_copy[\"age\"].round().astype(int)\n",
        "\n",
        "# we are only interested in survivors:\n",
        "survivors = titanic_copy[titanic_copy[\"survived\"] == 1]\n",
        "\n",
        "# encode income in the adult dataset\n",
        "income_mapping = {\"<=50K\": 0, \">50K\": 1}\n",
        "if \"income\" in adult_copy.columns:\n",
        "    adult_copy = adult_copy.assign(\n",
        "        income_encoded=adult_copy[\"income\"].map(income_mapping)\n",
        "    )\n",
        "\n",
        "# merge operation based on sex and rounded age\n",
        "merged_df = survivors.merge(adult_copy, on=[\"sex\", \"age\"], how=\"inner\")\n",
        "\n",
        "# mean salary of matched survivors\n",
        "gold_mean_salary = merged_df[\"income_encoded\"].mean() if not merged_df.empty else None\n",
        "\n",
        "print(\n",
        "    \"Estimated mean salary of Titanic survivors (based on adult dataset):\",\n",
        "    gold_mean_salary,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation:\n",
        "\n",
        "Based on the gold mean salary, we can observe that only ~16.3% of the matched Titanic survivors earned more than 50K, while the remaining 83.7% earned 50K or less (according to the adult dataset).\n",
        "\n",
        "(This result seems reasonable based on the history of the Titanic incident and given that it happened in 1912 (so salaries can't directly be compared to today's values))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```python\n",
            "import pandas as pd\n",
            "\n",
            "# Preprocess the adult dataset\n",
            "adult['sex'] = adult['gender'].str.lower()\n",
            "adult['age'] = adult['age'].round()\n",
            "adult['income'] = adult['income'].map({'<=50K': 0, '>50K': 1})\n",
            "\n",
            "# Preprocess the titanic dataset\n",
            "titanic['sex'] = titanic['Sex'].str.lower()\n",
            "titanic['age'] = titanic['Age'].round()\n",
            "\n",
            "# Merge the datasets on 'sex' and 'age'\n",
            "merged_df = pd.merge(titanic, adult, on=['sex', 'age'], how='inner')\n",
            "\n",
            "# Filter Titanic survivors and calculate their mean income\n",
            "survivors = merged_df[merged_df['Survived'] == 1]\n",
            "mean_income = survivors['income'].mean()\n",
            "\n",
            "output = mean_income\n",
            "```\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# code to reset dataframes (if needed) -> should be run if the code is run multiple times\n",
        "adult = pd.read_csv(\"hf://datasets/meghana/adult_income_dataset/adult.csv\")\n",
        "titanic = df.copy()\n",
        "\n",
        "example_prompt_template_new = \"\"\"Given access to two pandas dataframes named \"adult\" and \"titanic\", write the Python code to answer the user's question.\n",
        "The answer should be stored in a variable named \"output\". Don't put print statements in the code.\n",
        "Ensure the merge operation is performed correctly based on 'sex' and 'age'.\n",
        "- In the \"adult\" dataframe, 'sex' is mapped from 'gender'.\n",
        "- In the \"titanic\" dataframe, 'sex' is mapped from 'Sex'.\n",
        "- 'age' should be rounded to the nearest integer in both dataframes.\n",
        "Please note that column names in the \"titanic\" dataset are capitalized, while column names in the \"adult\" dataset are lowercase.\n",
        "Income in the \"adult\" dataset is given as either '<=50K' or '>50K' and should be mapped to numerical values (0 for '<=50K' and 1 for '>50K') before merging.\n",
        "Preprocess the \"adult\" dataset **before merging** by renaming 'gender' to 'sex' and rounding 'age' to the nearest integer.\n",
        "Preprocess the \"titanic\" dataset by renaming 'Sex' to 'sex' and rounding 'age' to the nearest integer to ensure consistent column names for merging.\n",
        "After merging, filter Titanic survivors and calculate their mean income.\n",
        "\\\n",
        "/\\\n",
        "\\\n",
        "\\\n",
        "\"{var_name_1}.head(5).to_string(index=False)\" as follows:\n",
        "{df_info_1}\n",
        "\\\n",
        "/\\\n",
        "\\\n",
        "\\\n",
        "\"{var_name_2}.head(5).to_string(index=False)\" as follows:\n",
        "{df_info_2}\n",
        "\\\n",
        "/\\\n",
        "\n",
        "Question: {user_question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = example_prompt_template_new.format(\n",
        "    var_name_1=\"adult\",\n",
        "    df_info_1=adult.head(5).to_string(index=False),\n",
        "    var_name_2=\"titanic\",\n",
        "    df_info_2=titanic.head(5).to_string(index=False),\n",
        "    user_question=\"What is the mean salary of titanic surviror based on adult dataset?\",\n",
        ")\n",
        "\n",
        "generated_answer = generate_answer(prompt)\n",
        "\n",
        "print(generated_answer)\n",
        "\n",
        "# Execute the generated answer and check if it's correct\n",
        "exec_answer(generated_answer, gold_mean_salary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result of the above code is \"True\", indicating that model succesfully generated code to answer the question \"What is the mean salary of titanic surviror based on adult dataset.\".\n",
        "\n",
        "The code should also print the code the LLM generated, which is the following:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Preprocess the adult dataset\n",
        "adult['sex'] = adult['gender'].str.lower()\n",
        "adult['age'] = adult['age'].round()\n",
        "adult['income'] = adult['income'].map({'<=50K': 0, '>50K': 1})\n",
        "\n",
        "# Preprocess the titanic dataset\n",
        "titanic['sex'] = titanic['Sex'].str.lower()\n",
        "titanic['age'] = titanic['Age'].round()\n",
        "\n",
        "# Merge the datasets on 'sex' and 'age'\n",
        "merged_df = pd.merge(titanic, adult, on=['sex', 'age'], how='inner')\n",
        "\n",
        "# Filter Titanic survivors and calculate their mean income\n",
        "survivors = merged_df[merged_df['Survived'] == 1]\n",
        "mean_income = survivors['income'].mean()\n",
        "\n",
        "output = mean_income\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".conda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
