{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 3: TableGPT\n",
        "\n",
        "\n",
        "In this lab, we'll discover the power of code generation models through TableGPT2. The aim is to see how the model can be used in data analysis.\n",
        "\n",
        "First of all, the notebook is divided into X sections:\n",
        "0. Installation: This section is dedicated to module installation, model loading and data loading.\n",
        "1. Guided introduction: Together, we'll discover how to use and evaluate TableGPT2.\n",
        "2. More questions: You'll need to add at least one new question type to our simple evaluation system.\n",
        "3. More data sets: You'll need to implement a question with multiple datasets.\n",
        "\n",
        "\n",
        "IMPORTANT:\n",
        "- You must work in pairs. You must submit **ONLY ONE NOTEBOOK** for each pair.\n",
        "- Do not share your work with other pairs.\n",
        "- You should not use Copilot, ChatGPT or similar tools. At the very least, remove the prompt ...\n",
        "- <font color='red'>All the things you need to do are indicated in red.</font>\n",
        "\n",
        "\n",
        "<font color='red'>**FIRST QUESTION:** What are the specificty of the TableGPT2 model?</font> https://huggingface.co/tablegpt/TableGPT2-7B"
      ],
      "metadata": {
        "id": "XOG3pXs8TxLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Setup"
      ],
      "metadata": {
        "id": "az4ycBgrXuDU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlTjE4nDH05G"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    GenerationConfig\n",
        ")\n",
        "\n",
        "import pandas as pd\n",
        "import torch"
      ],
      "metadata": {
        "id": "TYDBffMEX5Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU7OdU7mzyfN"
      },
      "outputs": [],
      "source": [
        "llm_name = \"tablegpt/TableGPT2-7B\"\n",
        "\n",
        "# We want to use 4bit quantization to save memory\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=False, load_in_4bit=True\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_name, padding_side=\"left\")\n",
        "# Prevent some transformers specific issues.\n",
        "tokenizer.use_default_system_prompt = False\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load LLM.\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    llm_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map={\"\": 0}, # load all the model layers on GPU 0\n",
        "    torch_dtype=torch.bfloat16, # float precision\n",
        ")\n",
        "# Set LLM on eval mode.\n",
        "llm.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(\n",
        "  max_new_tokens=512,\n",
        "  do_sample=False,\n",
        "  # do_sample=True,\n",
        "  # temperature=.7,\n",
        "  # top_p=.8,\n",
        "  # top_k=20,\n",
        "  eos_token_id=tokenizer.eos_token_id,\n",
        "  pad_token_id=tokenizer.pad_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "HAciK5cIWxgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME-E7zK-zZwU"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"hf://datasets/phihung/titanic/train.csv\")\n",
        "df = df.drop(\"Cabin\", axis=1).dropna()\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Guided Introduction: The Model.\n",
        "\n",
        "Below there is an example of a prompt that could be used with TableGPT2.\n",
        "\n",
        "```\n",
        "Given access to several pandas dataframes, write the Python code to answer the user's question.\n",
        "The answer should be store in a variable named \"output\".\n",
        "\n",
        "/*\n",
        "\"df.head(5).to_string(index=False)\" as follows:\n",
        " PassengerId  Survived  Pclass                                                Name    Sex  Age  SibSp  Parch           Ticket    Fare Embarked\n",
        "           1         0       3                             Braund, Mr. Owen Harris   male 22.0      1      0        A/5 21171  7.2500        S\n",
        "           2         1       1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38.0      1      0         PC 17599 71.2833        C\n",
        "           3         1       3                              Heikkinen, Miss. Laina female 26.0      0      0 STON/O2. 3101282  7.9250        S\n",
        "           4         1       1        Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0      1      0           113803 53.1000        S\n",
        "           5         0       3                            Allen, Mr. William Henry   male 35.0      0      0           373450  8.0500        S\n",
        "*/\n",
        "\n",
        "Question: How many child survive? (under 18)\n",
        "```\n",
        "\n",
        "The prompt is divided in 3 parts:\n",
        "1. The global instruction wich is to write python that could answer a question on a specific dataset.\n",
        "2. The header of the given dataset: 5 first lines of titanic dataset.\n",
        "3. The question to answer: \"How many child survive? (under 18)\n",
        "\n",
        "\n",
        "First, we will implement a function that generate an answer for this prompt.\n",
        "\n",
        "<font color='red'>TODO: Fill in the `generate_answer` function following the comments inside.</font>\n"
      ],
      "metadata": {
        "id": "YPxFK_SbYIGf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvUClWNx1jEj"
      },
      "outputs": [],
      "source": [
        "example_prompt_template = \"\"\"Given access to several pandas dataframes, write the Python code to answer the user's question.\n",
        "The answer should be store in a variable named \"output\".\n",
        "\n",
        "/*\n",
        "\"{var_name}.head(5).to_string(index=False)\" as follows:\n",
        "{df_info}\n",
        "*/\n",
        "\n",
        "Question: {user_question}\n",
        "\"\"\"\n",
        "\n",
        "def generate_answer(prompt, llm=llm, generation_config=generation_config):\n",
        "\n",
        "  # Create turns with the given prompt.\n",
        "\n",
        "  # Apply template with the tokenizer. Be careful to return pt tensors on the same device than `llm`.\n",
        "\n",
        "  # Generate with llm using the given generation config.\n",
        "\n",
        "  # Decode and select the answer to return.\n",
        "\n",
        "\n",
        "prompt = example_prompt_template.format(\n",
        "    var_name=\"df\",\n",
        "    df_info=df.head(5).to_string(index=False),\n",
        "    user_question=\"How many child survive? (under 18)\",\n",
        ")\n",
        "\n",
        "answer = generate_answer(prompt)\n",
        "\n",
        "print(prompt)\n",
        "print(\"\\n*****\\n\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Guided Introduction: The Answer.\n",
        "\n",
        "As you can see, the model answer with some generated code.\n",
        "\n",
        "```\n",
        "Python code:\n",
        "```python\n",
        "# Filter the dataframe to include only passengers under the age of 18\n",
        "children = df[df['Age'] < 18]\n",
        "\n",
        "# Count the number of children who survived\n",
        "child_survivors = children[children['Survived'] == 1]\n",
        "\n",
        "# Save the answer in the variable output\n",
        "output = len(child_survivors)\n",
        "```\n",
        "\n",
        "So we will need to execute it, but there is some difficulty:\n",
        "1. Sometime, the llm answer with \\`\\`\\`python ... \\`\\`\\`, sometime the llm answer directly with the code. We need to handle both cases.\n",
        "2. We need to recover the variable output from the execution.\n",
        "3. We need to evaluate single value and list of values.\n",
        "\n",
        "\n",
        "First, we will implement a function that generate an answer for this prompt.\n",
        "\n",
        "<font color='red'>TODO: Fill in the `exec_answer` function following the comments inside.</font>\n"
      ],
      "metadata": {
        "id": "8Nt90EerdwBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def exec_answer(answer, gold):\n",
        "\n",
        "  # Extract the code from the answer. Be careful, the code is now always in ``` ```.\n",
        "\n",
        "  # Execute the code, https://docs.python.org/3/library/functions.html#exec\n",
        "  # if the code work: Return True or False based on output == gold (be careful to handle iterable !)\n",
        "  # if the code don't work return False.\n",
        "\n",
        "\n",
        "print(exec_answer(answer, 61))"
      ],
      "metadata": {
        "id": "z_hwvxSGMSlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Guided Introduction: The Question.\n",
        "\n",
        "Now we want to automatically generate questions to evaluate the performance of our model. There are benchmarks on this subject, but here we want to practice code by generating the questions ourselves.\n",
        "\n",
        "We will generate some basic filter questions.\n",
        "\n",
        "<font color='red'>TODO: Fill in the `generate_filter_question` function following the comments inside.</font>\n"
      ],
      "metadata": {
        "id": "S1TrmGcQIEeI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il2vCMHRJ74t"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_random_question(generate_function, df, k=1, seed=42):\n",
        "  random.seed(seed)\n",
        "  return [generate_function(df) for _ in range(k)]\n",
        "\n",
        "def generate_filter_question(df):\n",
        "\n",
        "  # Create a question template that take a target colunm, a filter column and a filter value\n",
        "\n",
        "  # Get a random target column and a random filter column (be careful they should be differnts)\n",
        "\n",
        "  # Get a random filter value inside the filer column. Avoid NaN values.\n",
        "\n",
        "  # Compute the correct answer for the given target column, filter column and filter value.\n",
        "\n",
        "  # return formated question and associated answer in a dict {\"question\":[question], \"answer\":[answer]}\n",
        "\n",
        "generate_random_question(generate_filter_question, df, k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Guided Introduction: The Evaluation.\n",
        "\n",
        "The last step in this section is to evaluate our model on 20 random questions! We'll use simple accuracy.\n",
        "\n",
        "You should have an accuracy between 0.9 and 1.\n",
        "\n",
        "<font color='red'>TODO: Follow instruction in comment of the cell below.</font>\n",
        "\n",
        "<font color='green'>BONUS: Investigate on errors and improve our prompt/parsing to solve them.</font>\n"
      ],
      "metadata": {
        "id": "tzDJ9c3VhHt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Generate 20 random question\n",
        "\n",
        "# Iterate over question to format prompt, generate answer and execute answer.\n",
        "# Report the Accuracy\n",
        "\n",
        "print(\"Acc: \", )"
      ],
      "metadata": {
        "id": "xiE53oszRGcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. More Questions.\n",
        "\n",
        "Now it's your turn to imagine a type of question (\"How many ...\"). Implement a function to generate new type of question. Verify that our previous code work with your new question then evaluate it.\n",
        "\n",
        "<font color='red'>TODO: Generate **AT LEAST ONE** new type of question and report this new question accuracy.</font>\n"
      ],
      "metadata": {
        "id": "LGYL5LUeiEBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. More datasets.\n",
        "\n",
        "Below we load a new dataset: \"adult_income_dataset\".\n",
        "\n",
        "<font color='red'>TODO: Evaluate our questions on this new dataset. Report the accuracy. Comment Any differences.</font>\n",
        "\n",
        "<font color='green'>BONUS: Try to find a prompt that answer this question: What is the mean salary of titanic surviror based on adult dataset.</font>"
      ],
      "metadata": {
        "id": "mt35rcuMoAdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adult = pd.read_csv(\"hf://datasets/meghana/adult_income_dataset/adult.csv\")\n",
        "adult.info()\n",
        "\n",
        "titanic = df"
      ],
      "metadata": {
        "id": "z33vyj_Sk_WX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}